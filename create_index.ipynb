{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various loaders that one can use in LangChain.\n",
    "\n",
    "In particular, for PDFs one can use:\n",
    "\n",
    "- PyPDF: python library to extract text from PDFs. It works well when the file has a simple structure and there is no need of OCR;\n",
    "\n",
    "- Azure Document Intelligence: LangChain integration with Azure SDK. Document Intelligence is a Microsoft Azure service that enables the analysis, extraction, and structuring of data from unstructured documents including Word and PDFs, using advanced OCR. This tool is recommended when working with scanned or hand-written documents or documents containing tables or similar structures. Some feature include:\n",
    "\n",
    "    - **Prebuilt models** for common document types (e.g., invoices, receipts, IDs) and general documents.\n",
    "    \n",
    "    - **Custom models** that can be trained on specific document formats.\n",
    "    \n",
    "    - **Table** and **key-value** pair extraction for structured data retrieval.\n",
    "\n",
    "    - **Markdown** text extraction to easy document indexing.\n",
    "\n",
    "The model we are using is the `prebuild-layout` model. More info on this page [What is Azure AI Document Intelligence?](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-4.0.0).\n",
    "\n",
    "Note that, while document loading with PyPDF only takes a couple of seconds, using Document Intelligence is a little longer, as the model has to perform OCR on the document and extract tables and document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"<path_to_pdf>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyPDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Document Intelligence Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders.doc_intelligence import (\n",
    "    AzureAIDocumentIntelligenceLoader,\n",
    ")\n",
    "\n",
    "loader = AzureAIDocumentIntelligenceLoader(\n",
    "    api_endpoint=os.environ[\"DOC_INTELLIGENCE_ENDPOINT\"],\n",
    "    api_key=os.environ[\"DOC_INTELLIGENCE_KEY\"],\n",
    "    file_path=FILE_PATH,\n",
    ")\n",
    "\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several splitters are already developed in LangChain.\n",
    "\n",
    "- **CharacterTextSplitter**: is the simplest splitter. It splits the text based on a given chunk size (usually in token) and a give separator.\n",
    "\n",
    "- **RecursiveCharacterTextSplitter**: is more avdanced. It tries to recursively split the text on a give set of separators, by default `[\"\\n\\n\", \"\\n\", \" \", \"\"]`, observing a given chunk size. More info [here](https://python.langchain.com/docs/how_to/recursive_text_splitter/).\n",
    "\n",
    "- **MarkdownTextSplitter**: is specific for markdown text. It extends `RecursiveCharacterTextSplitter` using separators specific for Markdown, such as end of code blocks and markdown headings.\n",
    "\n",
    "- **MarkdownHeaderTextSplitter**: is specific for markdown text. It splits the text based on a list of tuples defining the headers. More info [here](https://python.langchain.com/docs/how_to/markdown_header_metadata_splitter/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter()\n",
    "splitted_docs = splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[(\"#\", \"Header 1\"), (\"##\", \"Header 2\")]\n",
    ")\n",
    "splitted_docs = splitter.split_text(document[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several indexes are available in LangChain and on the market. For this demo we are using **FAISS** index, which is open source.\n",
    "\n",
    "Complete list of supported indexes in LangChain [here](https://python.langchain.com/docs/integrations/vectorstores/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(splitted_docs, embeddings)\n",
    "vectorstore.save_local(\"pdf_index\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
