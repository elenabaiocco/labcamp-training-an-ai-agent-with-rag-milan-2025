{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Env Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist of all, we need to define the `State` of the graph, that is all the variables that are shared between the nodes.\n",
    "\n",
    "The `State` class defines all the attributes of the graph state, i.e. the elements all the nodes can access, use and modify, including the user question, the knowledge and the answer.  \n",
    "\n",
    "The state attributes in this case are:\n",
    "- `query`: the user query. It is used in the graph to read the user question.\n",
    "- `messages`: the list of intermediate steps of the agent's actions and observations. It contains the choices made by the model on which tool (if any) to use, the tool result and eventually also the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Annotated, List\n",
    "\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "    \"\"\"\n",
    "    Custom graph state to support agent-oriented decision making.\n",
    "    During each step, each node is able to add, modify, or extract the values from the state object.\n",
    "\n",
    "    Attributes:\n",
    "    - query (str): The user's question. Read-only.\n",
    "    - messages (List[AnyMessage]): The list of intermediate steps of the agent's actions and observations.\n",
    "            It will also contain the final answer.\n",
    "        NOTE: Annotated with add_messages to allow for adding steps to the agent-tools conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str\n",
    "    messages: Annotated[List[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `allow_dangerous_deserialization` parameter allows to load the index pikle file. Set it to `True` only to open trusted files, or files that you have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"pdf_index\", OpenAIEmbeddings(), allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tool definition, add a docstring that explains what the tool does and what the input is. The docstring will be included in the model calls so the model knows what all the tools are meant for and how to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(\"retrieve\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"\n",
    "    Query 'pdf_index' that contains info on TOPIC.\n",
    "    \n",
    "    Args:\n",
    "        - query (str): The search query.\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_PROMPT = \"\"\"You are a helpful assistant.\n",
    "Your task is to answer questions about TOPIC.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes on tool calling:\n",
    "\n",
    "- **Tool Creation**: use the tool function to create a tool, that is an association between a function and its schema.\n",
    "\n",
    "- **Tool Binding**: the tool is then connected to the model that supports tool calling (eg `gpt-4o`), so that the model is aware of the tool and the associated input schema. The method `.bind_tools()` is used to pass tool schemas to the model in subsequent invocations of the model.\n",
    "\n",
    "- **Tool Calling**: the model can decide to call the tool.\n",
    "\n",
    "- **Tool Execution**: the tool is executed with the arguments provided by the model.\n",
    "\n",
    "\n",
    "**NOTE**: OpenAI models allow for parallel tool calling by default. this means that the model can call multiple tools at a time. If you prefer to avoid this behaviour and force the model to call one tool at a time, you have to include the option \n",
    "\n",
    "```python\n",
    "parallel_tool_calls = False\n",
    "```\n",
    "\n",
    "when binding tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", AGENT_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "        (\"human\", \"{question}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools([retrieve], parallel_tool_calls=False)\n",
    "agent_chain = prompt | llm_with_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes and Edges Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "\n",
    "def run_query_agent(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Executes a query agent with a given state.\n",
    "    \"\"\"\n",
    "    messages = agent_chain.invoke(state)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "# Edges\n",
    "\n",
    "# no custom edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you add the nodes of the graph.\n",
    "\n",
    "Then the edges that link the nodes. There are two kinds of edges:\n",
    "- _simple_ edge: is a directional edge that defines the connection between a start and the end nodes;\n",
    "- _conditional_ edge: is a directional edge that routes the flow based on some conditions from a start node to some other nodes.\n",
    "\n",
    "In this case we defined one conditional esge that starts from the query agent and based on the agent output decides whether to end the flow or to call a tool.\n",
    "\n",
    "The _entry node_ defines the first node of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"tools\", ToolNode([retrieve]))\n",
    "graph.add_node(\"query_agent\", run_query_agent)\n",
    "\n",
    "\n",
    "# Add edges\n",
    "graph.set_entry_point(\"query_agent\")\n",
    "graph.add_conditional_edges(\n",
    "    source=\"query_agent\",  \n",
    "    path=tools_condition,\n",
    ")\n",
    "graph.add_edge(\"tools\", \"query_agent\")\n",
    "\n",
    "\n",
    "# Compile\n",
    "compiled_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        compiled_graph.get_graph().draw_mermaid_png(\n",
    "            output_file_path=\"graph.png\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compiled_graph.invoke(\n",
    "    {\"query\": \"Cosa significa Agile?\"}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
